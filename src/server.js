import 'dotenv/config';
import express from 'express';
import { Telegraf } from 'telegraf';
import OpenAI from 'openai';
import { handleUserText, handleCallback, startFlow } from './core/orchestrator.js';
import { mainMenu, backMenu } from './bot/keyboards.js';

const app = express();
const bot = new Telegraf(process.env.TELEGRAM_BOT_TOKEN);
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// ðŸ‘‰ ÐµÐ´Ð¸Ð½Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
const MODEL = process.env.OPENAI_MODEL || 'gpt-5-mini';

// ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² GPT
async function askGpt(input) {
  const r = await client.responses.create({
    model: MODEL,
    instructions:
      "You are GPT-5. If asked what model you are, answer exactly: 'GPT-5 Thinking'. Be concise and helpful.",
    input
  });
  return r.output_text || '';
}

// ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹/Ð¼ÐµÐ½ÑŽ
bot.command('menu', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));
bot.command('vin', startFlow('vin_flow'));
bot.command('oem', startFlow('oem_flow'));
bot.command('help', (ctx) =>
  ctx.reply('Ð¯ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð´Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ð¿Ð¾ VIN/OEM Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ GPT.', backMenu())
);

// Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸
bot.command('model', async (ctx) => {
  try {
    const test = await client.responses.create({ model: MODEL, input: 'pong' });
    await ctx.reply(`Using: ${MODEL}\nAPI responded with model: ${test.model || '(n/a)'}`);
  } catch (e) {
    await ctx.reply(`Model check error: ${e?.message || e}`);
  }
});

bot.hears('ðŸ”Ž ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð¿Ð¾ VIN', startFlow('vin_flow'));
bot.hears('ðŸ§© ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ OEM', startFlow('oem_flow'));
bot.hears('ðŸ¤– Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ðº GPT', (ctx) => ctx.reply('ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ GPT:', backMenu()));
bot.hears('ðŸ›’ ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð°', (ctx) => ctx.reply('ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð° Ð¿Ð¾ÐºÐ° Ð² Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ. Ð¡ÐºÐ¾Ñ€Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ð¼!', backMenu()));
bot.hears('â„¹ï¸ ÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ', (ctx) =>
  ctx.reply('ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ VIN (17 ÑÐ¸Ð¼Ð².) Ð¸Ð»Ð¸ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ» OEM. Ð›Ð¸Ð±Ð¾ Ð·Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ GPT.', backMenu())
);
bot.hears('â¬…ï¸ Ð’ Ð¼ÐµÐ½ÑŽ', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));

bot.on('callback_query', handleCallback);

bot.on('text', async (ctx) => {
  const handled = await handleUserText(ctx);
  if (handled) return;

  try {
    const answer = await askGpt(`ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ. Ð’Ð¾Ð¿Ñ€Ð¾Ñ: ${ctx.message.text}`);
    await ctx.reply(answer.slice(0, 4000), mainMenu());
  } catch (e) {
    console.error(e);
    await ctx.reply('Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° GPT. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·.', backMenu());
  }
});

app.use(express.json());

// healthcheck
app.get('/health', (_req, res) => res.send('OK'));

// webhook
const secret = process.env.WEBHOOK_SECRET || 'tg';
const path = `/tg/${secret}`;
app.use(path, bot.webhookCallback(path));

const port = Number(process.env.PORT) || 3000;
app.listen(port, '0.0.0.0', async () => {
  const base = process.env.WEBHOOK_URL || `http://localhost:${port}`;
  const url = `${base}${path}`;
  await bot.telegram.setWebhook(url);
  console.log('âœ… Webhook set:', url, ' model=', MODEL);
});
