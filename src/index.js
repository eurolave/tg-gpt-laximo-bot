import 'dotenv/config';
import { Telegraf } from 'telegraf';
import OpenAI from 'openai';
import { handleUserText, handleCallback, startFlow } from './core/orchestrator.js';
import { mainMenu, backMenu } from './bot/keyboards.js';

const bot = new Telegraf(process.env.TELEGRAM_BOT_TOKEN);
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// ðŸ‘‰ ÐµÐ´Ð¸Ð½Ð°Ñ Ñ‚Ð¾Ñ‡ÐºÐ° Ð²Ñ‹Ð±Ð¾Ñ€Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
const MODEL = process.env.OPENAI_MODEL || 'gpt-5-mini';

// ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² GPT
async function askGpt(input) {
  const r = await client.responses.create({
    model: MODEL,
    instructions:
      "You are GPT-5. If asked what model you are, answer exactly: 'GPT-5 Thinking'. Be concise and helpful.",
    input
  });
  return r.output_text || '';
}

// /start Ð¸ Ð±Ð°Ð·Ð¾Ð²Ð¾Ðµ Ð¼ÐµÐ½ÑŽ
bot.start(async (ctx) => {
  await ctx.reply('ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ, Ñ‡Ñ‚Ð¾ Ð½ÑƒÐ¶Ð½Ð¾:', mainMenu());
});

// ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹
bot.command('menu', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));
bot.command('vin', startFlow('vin_flow'));
bot.command('oem', startFlow('oem_flow'));
bot.command('help', (ctx) =>
  ctx.reply('Ð¯ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð´Ð¾Ð±Ñ€Ð°Ñ‚ÑŒ Ð´ÐµÑ‚Ð°Ð»Ð¸ Ð¿Ð¾ VIN/OEM Ð¸ Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ GPT.', backMenu())
);

// Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸
bot.command('model', async (ctx) => {
  try {
    const test = await client.responses.create({ model: MODEL, input: 'pong' });
    await ctx.reply(`Using: ${MODEL}\nAPI responded with model: ${test.model || '(n/a)'}`);
  } catch (e) {
    await ctx.reply(`Model check error: ${e?.message || e}`);
  }
});

// reply-ÐºÐ½Ð¾Ð¿ÐºÐ¸
bot.hears('ðŸ”Ž ÐŸÐ¾Ð´Ð±Ð¾Ñ€ Ð¿Ð¾ VIN', startFlow('vin_flow'));
bot.hears('ðŸ§© ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ OEM', startFlow('oem_flow'));
bot.hears('ðŸ¤– Ð’Ð¾Ð¿Ñ€Ð¾Ñ Ðº GPT', (ctx) => ctx.reply('ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ GPT:', backMenu()));
bot.hears('ðŸ›’ ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð°', (ctx) => ctx.reply('ÐšÐ¾Ñ€Ð·Ð¸Ð½Ð° Ð¿Ð¾ÐºÐ° Ð² Ñ€Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ. Ð¡ÐºÐ¾Ñ€Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ð¼!', backMenu()));
bot.hears('â„¹ï¸ ÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ', (ctx) =>
  ctx.reply('ÐžÑ‚Ð¿Ñ€Ð°Ð²ÑŒÑ‚Ðµ VIN (17 ÑÐ¸Ð¼Ð².) Ð¸Ð»Ð¸ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ» OEM. Ð›Ð¸Ð±Ð¾ Ð·Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ GPT.', backMenu())
);
bot.hears('â¬…ï¸ Ð’ Ð¼ÐµÐ½ÑŽ', (ctx) => ctx.reply('Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ Ð¼ÐµÐ½ÑŽ:', mainMenu()));

// Ð¸Ð½Ð»Ð°Ð¹Ð½-ÐºÐ½Ð¾Ð¿ÐºÐ¸
bot.on('callback_query', handleCallback);

// Ñ‚ÐµÐºÑÑ‚ â†’ ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²Ñ‹Ð¹ Ñ„Ð»Ð¾Ñƒ, Ð¸Ð½Ð°Ñ‡Ðµ GPT
bot.on('text', async (ctx) => {
  const handled = await handleUserText(ctx);
  if (handled) return;

  try {
    const answer = await askGpt(`ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ. Ð’Ð¾Ð¿Ñ€Ð¾Ñ: ${ctx.message.text}`);
    await ctx.reply(answer.slice(0, 4000), mainMenu());
  } catch (e) {
    console.error(e);
    await ctx.reply('Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° GPT. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ñ‘ Ñ€Ð°Ð·.', backMenu());
  }
});

bot.launch();
console.log('âœ… Bot started (long-polling, model=%s)', MODEL);

process.once('SIGINT', () => bot.stop('SIGINT'));
process.once('SIGTERM', () => bot.stop('SIGTERM'));
